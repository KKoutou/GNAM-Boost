{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d081cda4-9c70-4fc5-ad05-3de3043e6667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes:\n",
      "Xtr: (1526106, 31) Xva: (381469, 31) Xte: (476863, 31)\n",
      "Xtr_t: (1526106, 177) Xva_t: (381469, 177) Xte_t: (476863, 177)\n",
      "y: (1526106,) (381469,) (476863,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Load artifacts from preprocessing notebook\n",
    "\n",
    "import json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# dirs\n",
    "OUT_DIR = Path(r\"C:\\Users\\Nicee\\Desktop\\kenkyu\\gnamboost_dnbr_outputs\")\n",
    "INT_DIR = OUT_DIR / \"interim\"\n",
    "FIG_DIR = OUT_DIR / \"figs\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# raw X / y\n",
    "Xtr = joblib.load(INT_DIR / \"Xtr.pkl\")\n",
    "Xva = joblib.load(INT_DIR / \"Xva.pkl\")\n",
    "Xte = joblib.load(INT_DIR / \"Xte.pkl\")\n",
    "ytr = joblib.load(INT_DIR / \"ytr.npy\")\n",
    "yva = joblib.load(INT_DIR / \"yva.npy\")\n",
    "yte = joblib.load(INT_DIR / \"yte.npy\")\n",
    "\n",
    "# transformed X\n",
    "Xtr_t = joblib.load(INT_DIR / \"Xtr_t.npy\")\n",
    "Xva_t = joblib.load(INT_DIR / \"Xva_t.npy\")\n",
    "Xte_t = joblib.load(INT_DIR / \"Xte_t.npy\")\n",
    "\n",
    "# preprocessing and meta\n",
    "preproc = joblib.load(INT_DIR / \"preproc.joblib\")\n",
    "meta = json.load(open(INT_DIR / \"meta.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "feature_cols = meta[\"feature_cols\"]\n",
    "num_cols = meta[\"num_cols\"]\n",
    "bin_cols = meta[\"bin_cols\"]\n",
    "cat_cols = meta[\"cat_cols\"]\n",
    "\n",
    "df = pd.read_parquet(INT_DIR / \"df_filtered.parquet\")\n",
    "\n",
    "print(\"Loaded shapes:\")\n",
    "print(\"Xtr:\", Xtr.shape, \"Xva:\", Xva.shape, \"Xte:\", Xte.shape)\n",
    "print(\"Xtr_t:\", Xtr_t.shape, \"Xva_t:\", Xva_t.shape, \"Xte_t:\", Xte_t.shape)\n",
    "print(\"y:\", ytr.shape, yva.shape, yte.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4476bbf4-7594-4348-8c2b-6df826420104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic (3 vars): AUROC= 0.6216665557825451 AP= 0.023254804673634256\n",
      "XGB: AUROC= 0.7290596035392394 AP= 0.0835858929342947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Nicee\\\\Desktop\\\\kenkyu\\\\gnamboost_dnbr_outputs\\\\interim\\\\xgb_info.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: Logistic & XGBoost baselines\n",
    "\n",
    "import numpy as np, joblib\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "# logistic\n",
    "basic_cols = [\"IDADEMAE\", \"SEMAGESTAC\", \"PESO\"]\n",
    "Xtr_basic = Xtr[basic_cols].copy()\n",
    "Xva_basic = Xva[basic_cols].copy()\n",
    "Xte_basic = Xte[basic_cols].copy()\n",
    "\n",
    "pipe_logit = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\", StandardScaler()),\n",
    "])\n",
    "\n",
    "Xtr_logit = pipe_logit.fit_transform(Xtr_basic)\n",
    "Xva_logit = pipe_logit.transform(Xva_basic)\n",
    "Xte_logit = pipe_logit.transform(Xte_basic)\n",
    "\n",
    "logit = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=0.2, \n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=1000,\n",
    ")\n",
    "\n",
    "logit.fit(Xtr_logit, ytr)\n",
    "p_va_logit = logit.predict_proba(Xva_logit)[:, 1]\n",
    "p_te_logit_raw = logit.predict_proba(Xte_logit)[:, 1]\n",
    "\n",
    "iso_logit = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_logit, yva)\n",
    "p_logit = iso_logit.transform(p_te_logit_raw)\n",
    "\n",
    "print(\"Logistic (3 vars): AUROC=\",\n",
    "      roc_auc_score(yte, p_logit),\n",
    "      \"AP=\",\n",
    "      average_precision_score(yte, p_logit))\n",
    "\n",
    "# XGBoost\n",
    "pos_rate_all = float(ytr.mean())\n",
    "scale_pos = (1.0 - pos_rate_all) / max(pos_rate_all, 1e-6)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    grow_policy=\"lossguide\",\n",
    "    max_depth=0,\n",
    "    max_leaves=80,\n",
    "    learning_rate=0.02,\n",
    "    n_estimators=12000,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    colsample_bylevel=0.8,\n",
    "    min_child_weight=10,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=2.0,\n",
    "    gamma=0.0,\n",
    "    max_bin=256,\n",
    "    max_delta_step=1,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=meta[\"seed\"],\n",
    "    early_stopping_rounds=1000,\n",
    "    eval_metric=\"auc\",\n",
    "    scale_pos_weight=scale_pos,\n",
    ")\n",
    "\n",
    "xgb_model.fit(Xtr_t, ytr, eval_set=[(Xva_t, yva)], verbose=False)\n",
    "best_iter = xgb_model.best_iteration\n",
    "booster = xgb_model.get_booster()\n",
    "\n",
    "dva = xgb.DMatrix(Xva_t)\n",
    "dte = xgb.DMatrix(Xte_t)\n",
    "\n",
    "p_va_xgb = booster.predict(dva, iteration_range=(0, best_iter + 1))\n",
    "p_te_xgb_raw = booster.predict(dte, iteration_range=(0, best_iter + 1))\n",
    "\n",
    "iso_xgb = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_xgb, yva)\n",
    "p_xgb = iso_xgb.transform(p_te_xgb_raw)\n",
    "\n",
    "print(\"XGB: AUROC=\",\n",
    "      roc_auc_score(yte, p_xgb),\n",
    "      \"AP=\",\n",
    "      average_precision_score(yte, p_xgb))\n",
    "\n",
    "# save\n",
    "joblib.dump(p_logit, INT_DIR / \"p_logit.npy\")\n",
    "joblib.dump(p_xgb,   INT_DIR / \"p_xgb.npy\")\n",
    "joblib.dump(logit,   INT_DIR / \"logit_model.joblib\")\n",
    "booster.save_model(str(INT_DIR / \"xgb_booster.json\"))\n",
    "joblib.dump({\"best_iteration\": best_iter}, INT_DIR / \"xgb_info.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd7737d-0d99-409f-a94e-81c0fa8d3db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010/180 | train_loss=0.05802 | val_loss=0.05810 | AUROC=0.7098 | AP=0.0658 | best_AUROC=0.7101 | bad_epochs=2 | time=78.0s\n",
      "Epoch 020/180 | train_loss=0.05735 | val_loss=0.05793 | AUROC=0.7099 | AP=0.0673 | best_AUROC=0.7113 | bad_epochs=0 | time=78.3s\n",
      "Epoch 030/180 | train_loss=0.05654 | val_loss=0.05801 | AUROC=0.7093 | AP=0.0673 | best_AUROC=0.7113 | bad_epochs=10 | time=77.9s\n",
      "Epoch 040/180 | train_loss=0.05713 | val_loss=0.05785 | AUROC=0.7099 | AP=0.0674 | best_AUROC=0.7113 | bad_epochs=20 | time=107.5s\n",
      "Epoch 050/180 | train_loss=0.05657 | val_loss=0.05791 | AUROC=0.7105 | AP=0.0681 | best_AUROC=0.7113 | bad_epochs=30 | time=107.3s\n",
      "Early stop @ epoch 59, best_AUROC=0.71128\n",
      "GNAM valid (raw): AUROC= 0.711283237056559 AP= 0.06721903696098361\n",
      "GNAM test (calib): AUROC= 0.7148797820758007 AP= 0.0708614318580256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Nicee\\\\Desktop\\\\kenkyu\\\\gnamboost_dnbr_outputs\\\\interim\\\\p_gnam.npy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10: GNAM training\n",
    "\n",
    "import os, contextlib, time\n",
    "import numpy as np, joblib, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"12\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"12\"\n",
    "try:\n",
    "    torch.set_num_threads(12)\n",
    "    torch.set_num_interop_threads(4)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def get_amp_ctx():\n",
    "    try:\n",
    "        return torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16)\n",
    "    except Exception:\n",
    "        return contextlib.nullcontext()\n",
    "\n",
    "amp_ctx = get_amp_ctx()\n",
    "\n",
    "class ExULayer(nn.Module):\n",
    "    def __init__(self, nonlin=\"softplus\"):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.zeros(1))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "        self.nonlin = nonlin\n",
    "    def forward(self, x):\n",
    "        z = torch.exp(self.w) * (x - self.b)\n",
    "        if self.nonlin == \"softplus\":\n",
    "            return F.softplus(z)\n",
    "        if self.nonlin == \"tanh\":\n",
    "            return torch.tanh(z)\n",
    "        return F.relu(z)\n",
    "\n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self, hidden=96, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.exu = ExULayer(\"softplus\")\n",
    "        self.fc1 = nn.Linear(1, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden // 2)\n",
    "        self.fc3 = nn.Linear(hidden // 2, 1)\n",
    "        self.dp = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        h = self.exu(x)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.dp(F.relu(self.fc2(h)))\n",
    "        return self.fc3(h)\n",
    "\n",
    "class GNAM(nn.Module):\n",
    "    def __init__(self, n_features, hidden=96, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.fnets = nn.ModuleList([FeatureNet(hidden, dropout) for _ in range(n_features)])\n",
    "    def forward(self, x):\n",
    "        outs = [self.fnets[j](x[:, j:j+1]) for j in range(x.shape[1])]\n",
    "        eta = self.bias + torch.stack(outs, dim=2).sum(dim=2)\n",
    "        return eta.squeeze(1)\n",
    "\n",
    "def to_loader(X, y, batch=4096, shuffle=True):\n",
    "    ds = TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
    "                       torch.tensor(y, dtype=torch.float32))\n",
    "    return DataLoader(ds, batch_size=batch, shuffle=shuffle,\n",
    "                      drop_last=False, num_workers=0)\n",
    "\n",
    "def train_gnam(model, Xtr_mat, ytr_vec, Xva_mat, yva_vec,\n",
    "               max_epochs=180, lr=7e-4, weight_decay=1e-6,\n",
    "               patience=40, device=\"cpu\", pos_weight=None,\n",
    "               logit_clamp=None, grad_clip_norm=3.0, report_every=10):\n",
    "\n",
    "    if pos_weight is not None:\n",
    "        criterion = nn.BCEWithLogitsLoss(\n",
    "            pos_weight=torch.tensor(float(pos_weight), device=device)\n",
    "        )\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    tr_loader = to_loader(Xtr_mat, ytr_vec, batch=4096, shuffle=True)\n",
    "    va_loader = to_loader(Xva_mat, yva_vec, batch=4096, shuffle=False)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_metric = -np.inf\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for ep in range(max_epochs):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        tr_loss_sum, tr_n = 0.0, 0\n",
    "\n",
    "        for xb, yb in tr_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with amp_ctx:\n",
    "                logits = model(xb)\n",
    "                if logit_clamp is not None:\n",
    "                    logits = torch.clamp(logits, -abs(logit_clamp), abs(logit_clamp))\n",
    "                loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            if grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "            opt.step()\n",
    "            tr_loss_sum += float(loss.item()) * len(xb)\n",
    "            tr_n += len(xb)\n",
    "\n",
    "        tr_loss = tr_loss_sum / max(tr_n, 1)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        va_loss_sum, va_n = 0.0, 0\n",
    "        logits_all, y_all = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                with amp_ctx:\n",
    "                    logits = model(xb)\n",
    "                    if logit_clamp is not None:\n",
    "                        logits = torch.clamp(logits, -abs(logit_clamp), abs(logit_clamp))\n",
    "                vloss = F.binary_cross_entropy_with_logits(logits, yb).item()\n",
    "                va_loss_sum += vloss * len(xb)\n",
    "                va_n += len(xb)\n",
    "                logits_all.append(logits.detach().cpu().numpy().ravel())\n",
    "                y_all.append(yb.detach().cpu().numpy().ravel())\n",
    "\n",
    "        avg_val_loss = va_loss_sum / max(va_n, 1)\n",
    "        y_cat = np.concatenate(y_all) if y_all else np.array([])\n",
    "        p_cat = 1.0 / (1.0 + np.exp(-np.concatenate(logits_all))) if logits_all else np.array([])\n",
    "\n",
    "        if y_cat.size > 0 and np.unique(y_cat).size > 1:\n",
    "            auroc = roc_auc_score(y_cat, p_cat)\n",
    "            ap = average_precision_score(y_cat, p_cat)\n",
    "        else:\n",
    "            auroc, ap = np.nan, np.nan\n",
    "\n",
    "        metric = auroc if np.isfinite(auroc) else -np.inf\n",
    "\n",
    "        if ((ep + 1) % report_every) == 0:\n",
    "            print(\n",
    "                f\"Epoch {ep+1:03d}/{max_epochs} | \"\n",
    "                f\"train_loss={tr_loss:.5f} | val_loss={avg_val_loss:.5f} | \"\n",
    "                f\"AUROC={auroc:.4f} | AP={ap:.4f} | \"\n",
    "                f\"best_AUROC={best_metric:.4f} | bad_epochs={bad_epochs} | \"\n",
    "                f\"time={time.time()-t0:.1f}s\"\n",
    "            )\n",
    "\n",
    "        # early stop\n",
    "        if metric > best_metric + 1e-4:\n",
    "            best_metric = metric\n",
    "            bad_epochs = 0\n",
    "            best_state = {k: v.detach().cpu().clone()\n",
    "                          for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(f\"Early stop @ epoch {ep+1}, best_AUROC={best_metric:.5f}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "SUB_N = 400_000\n",
    "if Xtr_t.shape[0] > SUB_N:\n",
    "    rng = np.random.RandomState(meta[\"seed\"])\n",
    "    idx_sub = rng.choice(Xtr_t.shape[0], size=SUB_N, replace=False)\n",
    "    Xtr_t_sub = Xtr_t[idx_sub]\n",
    "    ytr_sub = ytr[idx_sub]\n",
    "else:\n",
    "    Xtr_t_sub, ytr_sub = Xtr_t, ytr\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "gnam = GNAM(n_features=Xtr_t.shape[1], hidden=96, dropout=0.05)\n",
    "gnam = train_gnam(\n",
    "    gnam,\n",
    "    Xtr_t_sub, ytr_sub,\n",
    "    Xva_t, yva,\n",
    "    max_epochs=180,\n",
    "    lr=7e-4,\n",
    "    weight_decay=1e-6,\n",
    "    patience=40,\n",
    "    device=device,\n",
    "    pos_weight=None,\n",
    "    logit_clamp=None,\n",
    "    grad_clip_norm=3.0,\n",
    "    report_every=10,\n",
    ")\n",
    "\n",
    "torch.save(gnam.state_dict(), INT_DIR / \"gnam_best.pt\")\n",
    "\n",
    "# predictions\n",
    "import scipy.special as sps\n",
    "\n",
    "gnam.eval()\n",
    "with torch.no_grad(), amp_ctx:\n",
    "    eta_tr = gnam(torch.tensor(Xtr_t, dtype=torch.float32, device=device)).cpu().numpy().astype(np.float32)\n",
    "    eta_va = gnam(torch.tensor(Xva_t, dtype=torch.float32, device=device)).cpu().numpy().astype(np.float32)\n",
    "    eta_te = gnam(torch.tensor(Xte_t, dtype=torch.float32, device=device)).cpu().numpy().astype(np.float32)\n",
    "\n",
    "p_tr_gnam = sps.expit(eta_tr)\n",
    "p_va_gnam = sps.expit(eta_va)\n",
    "p_te_gnam = sps.expit(eta_te)\n",
    "\n",
    "iso_gnam = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_gnam, yva)\n",
    "p_gnam = iso_gnam.transform(p_te_gnam)\n",
    "\n",
    "print(\"GNAM valid (raw): AUROC=\",\n",
    "      roc_auc_score(yva, p_va_gnam),\n",
    "      \"AP=\",\n",
    "      average_precision_score(yva, p_va_gnam))\n",
    "print(\"GNAM test (calib): AUROC=\",\n",
    "      roc_auc_score(yte, p_gnam),\n",
    "      \"AP=\",\n",
    "      average_precision_score(yte, p_gnam))\n",
    "\n",
    "joblib.dump({\"eta_tr\": eta_tr, \"eta_va\": eta_va, \"eta_te\": eta_te}, INT_DIR / \"gnam_eta.joblib\")\n",
    "joblib.dump(p_gnam, INT_DIR / \"p_gnam.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11be11c4-ed97-4509-9c59-f5d24aecb46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1526106 | pos_rate: 0.011275101467394794\n",
      "[0]\ttrain-aucpr:0.07696\ttrain-auc:0.71951\ttrain-logloss:0.05665\tvalid-aucpr:0.06834\tvalid-auc:0.71193\tvalid-logloss:0.05770\n",
      "[200]\ttrain-aucpr:0.14195\ttrain-auc:0.78817\ttrain-logloss:0.05293\tvalid-aucpr:0.08850\tvalid-auc:0.72332\tvalid-logloss:0.05679\n",
      "[400]\ttrain-aucpr:0.18402\ttrain-auc:0.83373\ttrain-logloss:0.05054\tvalid-aucpr:0.08927\tvalid-auc:0.72381\tvalid-logloss:0.05676\n",
      "[600]\ttrain-aucpr:0.23026\ttrain-auc:0.86619\ttrain-logloss:0.04841\tvalid-aucpr:0.08902\tvalid-auc:0.72213\tvalid-logloss:0.05682\n",
      "[800]\ttrain-aucpr:0.27837\ttrain-auc:0.88997\ttrain-logloss:0.04649\tvalid-aucpr:0.08866\tvalid-auc:0.72122\tvalid-logloss:0.05687\n",
      "[1000]\ttrain-aucpr:0.32760\ttrain-auc:0.90849\ttrain-logloss:0.04472\tvalid-aucpr:0.08846\tvalid-auc:0.71956\tvalid-logloss:0.05693\n",
      "[1200]\ttrain-aucpr:0.37749\ttrain-auc:0.92332\ttrain-logloss:0.04306\tvalid-aucpr:0.08804\tvalid-auc:0.71804\tvalid-logloss:0.05701\n",
      "[1379]\ttrain-aucpr:0.42220\ttrain-auc:0.93507\ttrain-logloss:0.04160\tvalid-aucpr:0.08767\tvalid-auc:0.71678\tvalid-logloss:0.05708\n",
      "Best iteration: 380\n",
      "GNAM–Boost (valid raw): AUROC = 0.7239  AP = 0.0894\n",
      "GNAM–Boost (test calib): AUROC = 0.7272  AP = 0.0932  Brier = 0.0111  LogLoss = 0.0576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Nicee\\\\Desktop\\\\kenkyu\\\\gnamboost_dnbr_outputs\\\\interim\\\\p_final.npy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 11: GNAM–Boost\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "import numpy as np, joblib\n",
    "\n",
    "# load GNAM logits\n",
    "etas = joblib.load(INT_DIR / \"gnam_eta.joblib\")\n",
    "eta_tr, eta_va, eta_te = etas[\"eta_tr\"], etas[\"eta_va\"], etas[\"eta_te\"]\n",
    "\n",
    "# DMatrices with base_margin from GNAM\n",
    "dtr2 = xgb.DMatrix(Xtr_t, label=ytr)\n",
    "dtr2.set_base_margin(eta_tr.astype(np.float32))\n",
    "\n",
    "dva2 = xgb.DMatrix(Xva_t, label=yva)\n",
    "dva2.set_base_margin(eta_va.astype(np.float32))\n",
    "\n",
    "dte2 = xgb.DMatrix(Xte_t, label=yte)\n",
    "dte2.set_base_margin(eta_te.astype(np.float32))\n",
    "\n",
    "pos_rate = float(ytr.mean())\n",
    "print(\"Train size:\", dtr2.num_row(), \"| pos_rate:\", pos_rate)\n",
    "\n",
    "params2 = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"aucpr\", \"auc\", \"logloss\"],\n",
    "    \"grow_policy\": \"lossguide\",\n",
    "    \"max_depth\": 0,\n",
    "    \"max_leaves\": 128,\n",
    "    \"eta\": 0.03,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"colsample_bylevel\": 0.9,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"max_delta_step\": 0,\n",
    "    \"scale_pos_weight\": 1.0,\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n",
    "\n",
    "bst2 = xgb.train(\n",
    "    params2,\n",
    "    dtr2,\n",
    "    num_boost_round=8000,\n",
    "    evals=[(dtr2, \"train\"), (dva2, \"valid\")],\n",
    "    early_stopping_rounds=1000,\n",
    "    verbose_eval=200\n",
    ")\n",
    "\n",
    "best_iter = bst2.best_iteration if hasattr(bst2, \"best_iteration\") else None\n",
    "print(\"Best iteration:\", best_iter)\n",
    "\n",
    "# raw predictions\n",
    "p_va_gb_raw = bst2.predict(\n",
    "    dva2,\n",
    "    iteration_range=(0, best_iter + 1) if best_iter is not None else None\n",
    ")\n",
    "p_te_gb_raw = bst2.predict(\n",
    "    dte2,\n",
    "    iteration_range=(0, best_iter + 1) if best_iter is not None else None\n",
    ")\n",
    "\n",
    "print(\"GNAM–Boost (valid raw): AUROC = {:.4f}  AP = {:.4f}\".format(\n",
    "    roc_auc_score(yva, p_va_gb_raw),\n",
    "    average_precision_score(yva, p_va_gb_raw)\n",
    "))\n",
    "\n",
    "# isotonic calibration on valid\n",
    "iso_gb = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_gb_raw, yva)\n",
    "p_final = iso_gb.transform(p_te_gb_raw)\n",
    "\n",
    "print(\n",
    "    \"GNAM–Boost (test calib): AUROC = {:.4f}  AP = {:.4f}  Brier = {:.4f}  LogLoss = {:.4f}\"\n",
    "    .format(\n",
    "        roc_auc_score(yte, p_final),\n",
    "        average_precision_score(yte, p_final),\n",
    "        brier_score_loss(yte, p_final),\n",
    "        log_loss(yte, p_final)\n",
    "    )\n",
    ")\n",
    "\n",
    "# save artifacts\n",
    "bst2.save_model(str(INT_DIR / \"gnamboost_bst.json\"))\n",
    "joblib.dump(p_final, INT_DIR / \"p_final.npy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
