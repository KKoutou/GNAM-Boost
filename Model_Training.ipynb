{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b484c20-83fd-49dc-a220-cd961ea5e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Artifacts\n",
    "import json, joblib, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\Users\\Nicee\\Desktop\\kenkyu\\gnamboost_outputs\")\n",
    "INT_DIR = OUT_DIR / \"interim\"\n",
    "FIG_DIR = OUT_DIR / \"figs\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Xtr = joblib.load(INT_DIR/\"Xtr.pkl\"); Xva = joblib.load(INT_DIR/\"Xva.pkl\"); Xte = joblib.load(INT_DIR/\"Xte.pkl\")\n",
    "ytr = joblib.load(INT_DIR/\"ytr.npy\"); yva = joblib.load(INT_DIR/\"yva.npy\"); yte = joblib.load(INT_DIR/\"yte.npy\")\n",
    "Xtr_t = joblib.load(INT_DIR/\"Xtr_t.npy\"); Xva_t = joblib.load(INT_DIR/\"Xva_t.npy\"); Xte_t = joblib.load(INT_DIR/\"Xte_t.npy\")\n",
    "preproc = joblib.load(INT_DIR/\"preproc.joblib\")\n",
    "meta = json.load(open(INT_DIR/\"meta.json\",\"r\",encoding=\"utf-8\"))\n",
    "\n",
    "feature_cols = meta[\"feature_cols\"]; num_cols = meta[\"num_cols\"]; bin_cols = meta[\"bin_cols\"]; cat_cols = meta[\"cat_cols\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd211e4-2d91-4e5c-97e5-1737a171b55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic: AUROC= 0.7795568561576975  AP= 0.6232944200494668\n",
      "XGB: AUROC= 0.7984220843281925  AP= 0.661829642863953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Nicee\\\\Desktop\\\\kenkyu\\\\gnamboost_outputs\\\\interim\\\\xgb_info.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 Logistic & XGBoost Baselines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Logistic\n",
    "logit = LogisticRegression(penalty=\"l2\", C=0.001, solver=\"lbfgs\", max_iter=2000)\n",
    "logit.fit(Xtr_t, ytr)\n",
    "p_va_logit = logit.predict_proba(Xva_t)[:,1]\n",
    "p_te_logit_raw = logit.predict_proba(Xte_t)[:,1]\n",
    "iso_logit = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_logit, yva)\n",
    "p_logit = iso_logit.transform(p_te_logit_raw)\n",
    "print(\"Logistic: AUROC=\", roc_auc_score(yte,p_logit), \" AP=\", average_precision_score(yte,p_logit))\n",
    "\n",
    "# XGBoost + early stop\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    grow_policy=\"lossguide\", max_depth=0, max_leaves=64,\n",
    "    learning_rate=0.01, n_estimators=20000, subsample=0.75,\n",
    "    colsample_bytree=0.75, colsample_bylevel=0.85,\n",
    "    min_child_weight=5, reg_alpha=0.15, reg_lambda=2.5,\n",
    "    gamma=0.0, max_bin=256, max_delta_step=1,\n",
    "    tree_method=\"hist\", n_jobs=-1, random_state=meta[\"seed\"],\n",
    "    early_stopping_rounds=1200,\n",
    "    eval_metric=\"auc\"\n",
    ")\n",
    "xgb_model.fit(Xtr_t, ytr, eval_set=[(Xva_t,yva)], verbose=False)\n",
    "best_iter = xgb_model.best_iteration\n",
    "booster = xgb_model.get_booster()\n",
    "dva = xgb.DMatrix(Xva_t); dte = xgb.DMatrix(Xte_t)\n",
    "p_va_xgb = booster.predict(dva, iteration_range=(0, best_iter+1))\n",
    "p_te_xgb_raw = booster.predict(dte, iteration_range=(0, best_iter+1))\n",
    "iso_xgb = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_xgb, yva)\n",
    "p_xgb = iso_xgb.transform(p_te_xgb_raw)\n",
    "print(\"XGB: AUROC=\", roc_auc_score(yte,p_xgb), \" AP=\", average_precision_score(yte,p_xgb))\n",
    "\n",
    "# Save predictions & models\n",
    "joblib.dump(p_logit, INT_DIR/\"p_logit.npy\")\n",
    "joblib.dump(p_xgb,   INT_DIR/\"p_xgb.npy\")\n",
    "joblib.dump(logit,   INT_DIR/\"logit_model.joblib\")\n",
    "booster.save_model(str(INT_DIR/\"xgb_booster.json\"))\n",
    "joblib.dump({\"best_iteration\": best_iter}, INT_DIR/\"xgb_info.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e910cd6-2e04-4524-9b3d-b482e0700d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020/600 | train_loss=0.73667 | val_loss=0.57059 | AUROC=0.7843 | AP=0.6322 | bad_epochs=16 | time=25.6s\n",
      "Epoch 040/600 | train_loss=0.73645 | val_loss=0.55465 | AUROC=0.7842 | AP=0.6329 | bad_epochs=36 | time=23.7s\n",
      "Epoch 060/600 | train_loss=0.73548 | val_loss=0.55984 | AUROC=0.7843 | AP=0.6325 | bad_epochs=56 | time=23.9s\n",
      "Epoch 080/600 | train_loss=0.73498 | val_loss=0.56118 | AUROC=0.7849 | AP=0.6340 | bad_epochs=76 | time=24.0s\n",
      "Epoch 100/600 | train_loss=0.73530 | val_loss=0.57030 | AUROC=0.7845 | AP=0.6335 | bad_epochs=96 | time=24.1s\n",
      "Epoch 120/600 | train_loss=0.73493 | val_loss=0.55416 | AUROC=0.7848 | AP=0.6339 | bad_epochs=116 | time=24.2s\n",
      "Epoch 140/600 | train_loss=0.73500 | val_loss=0.55810 | AUROC=0.7847 | AP=0.6339 | bad_epochs=136 | time=24.4s\n",
      "Epoch 160/600 | train_loss=0.73496 | val_loss=0.56936 | AUROC=0.7849 | AP=0.6347 | bad_epochs=156 | time=24.1s\n",
      "Epoch 180/600 | train_loss=0.73514 | val_loss=0.57015 | AUROC=0.7843 | AP=0.6330 | bad_epochs=176 | time=24.4s\n",
      "Epoch 200/600 | train_loss=0.73486 | val_loss=0.56435 | AUROC=0.7850 | AP=0.6340 | bad_epochs=196 | time=24.1s\n",
      "Early stop @ epoch 203, best_val_loss=0.53404\n",
      "GNAM raw (valid AUROC/AP): 0.7832670109122277 0.6302402875763038\n",
      "GNAM test (raw AUROC/AP): 0.7822885111668445 0.6316023057936705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Nicee\\\\Desktop\\\\kenkyu\\\\gnamboost_outputs\\\\interim\\\\p_gnam.npy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 GNAM training\n",
    "import os, math, numpy as np, contextlib, torch, joblib\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import scipy.special as sps\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"12\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"12\"\n",
    "try:\n",
    "    torch.set_num_threads(12); torch.set_num_interop_threads(4)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def get_amp_ctx():\n",
    "    try:\n",
    "        return torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16)\n",
    "    except Exception:\n",
    "        return contextlib.nullcontext()\n",
    "amp_ctx = get_amp_ctx()\n",
    "\n",
    "class ExULayer(nn.Module):\n",
    "    def __init__(self, nonlin=\"softplus\"):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.zeros(1))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "        self.nonlin = nonlin\n",
    "    def forward(self, x):\n",
    "        z = torch.exp(self.w) * (x - self.b)\n",
    "        if self.nonlin == \"softplus\": return F.softplus(z)\n",
    "        if self.nonlin == \"tanh\": return torch.tanh(z)\n",
    "        return F.relu(z)\n",
    "\n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self, hidden=96, dropout=0.20):\n",
    "        super().__init__()\n",
    "        self.exu = ExULayer(\"softplus\")\n",
    "        self.fc1 = nn.Linear(1, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden // 2)\n",
    "        self.fc3 = nn.Linear(hidden // 2, 1)\n",
    "        self.dp = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        h = self.exu(x)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.dp(F.relu(self.fc2(h)))\n",
    "        return self.fc3(h)\n",
    "\n",
    "class GNAM(nn.Module):\n",
    "    def __init__(self, n_features, hidden=96, dropout=0.20):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.fnets = nn.ModuleList([FeatureNet(hidden, dropout) for _ in range(n_features)])\n",
    "    def forward(self, x):\n",
    "        outs = [self.fnets[j](x[:, j:j+1]) for j in range(x.shape[1])]\n",
    "        eta = self.bias + torch.stack(outs, dim=2).sum(dim=2)\n",
    "        return eta.squeeze(1)\n",
    "\n",
    "def to_loader(X, y, batch=1024, shuffle=True):\n",
    "    ds = TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
    "                       torch.tensor(y, dtype=torch.float32))\n",
    "    return DataLoader(ds, batch_size=batch, shuffle=shuffle, drop_last=False, num_workers=0)\n",
    "\n",
    "def train_gnam(model, tr_loader, va_loader, max_epochs=600, lr=5e-4, weight_decay=1e-4, patience=200,\n",
    "               device=\"cpu\", pos_weight=None, logit_clamp=10.0, grad_clip_norm=5.0, report_every=20):\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = (nn.BCEWithLogitsLoss(pos_weight=torch.tensor(float(pos_weight), device=device))\n",
    "                 if pos_weight is not None else nn.BCEWithLogitsLoss())\n",
    "    best_loss, bad_epochs, best_state = np.inf, 0, None\n",
    "    model.to(device)\n",
    "\n",
    "    for ep in range(max_epochs):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "        tr_loss_sum, tr_n = 0.0, 0\n",
    "        for xb, yb in tr_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with amp_ctx:\n",
    "                logits = model(xb)\n",
    "                if logit_clamp is not None:\n",
    "                    logits = torch.clamp(logits, -abs(logit_clamp), abs(logit_clamp))\n",
    "                loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            if grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "            opt.step()\n",
    "            tr_loss_sum += float(loss.item()) * len(xb)\n",
    "            tr_n += len(xb)\n",
    "        tr_loss = tr_loss_sum / max(tr_n, 1)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        va_loss_sum, va_n = 0.0, 0\n",
    "        logits_all, y_all = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                with amp_ctx:\n",
    "                    logits = model(xb)\n",
    "                    if logit_clamp is not None:\n",
    "                        logits = torch.clamp(logits, -abs(logit_clamp), abs(logit_clamp))\n",
    "                    vloss = F.binary_cross_entropy_with_logits(logits, yb).item()\n",
    "                va_loss_sum += vloss * len(xb)\n",
    "                va_n += len(xb)\n",
    "                logits_all.append(logits.detach().cpu().numpy().ravel())\n",
    "                y_all.append(yb.detach().cpu().numpy().ravel())\n",
    "        avg_val_loss = va_loss_sum / max(va_n, 1)\n",
    "\n",
    "        # Metrics on validation\n",
    "        y_cat = np.concatenate(y_all) if y_all else np.array([])\n",
    "        p_cat = 1.0 / (1.0 + np.exp(-np.concatenate(logits_all))) if logits_all else np.array([])\n",
    "        auroc = roc_auc_score(y_cat, p_cat) if len(np.unique(y_cat)) > 1 else np.nan\n",
    "        ap = average_precision_score(y_cat, p_cat) if len(np.unique(y_cat)) > 1 else np.nan\n",
    "\n",
    "        # Report every N epochs\n",
    "        if ((ep + 1) % report_every) == 0:\n",
    "            print(f\"Epoch {ep+1:03d}/{max_epochs} | \"\n",
    "                  f\"train_loss={tr_loss:.5f} | val_loss={avg_val_loss:.5f} | \"\n",
    "                  f\"AUROC={auroc:.4f} | AP={ap:.4f} | \"\n",
    "                  f\"bad_epochs={bad_epochs} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        # Early stop\n",
    "        if avg_val_loss < best_loss - 1e-6:\n",
    "            best_loss, bad_epochs = avg_val_loss, 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(f\"Early stop @ epoch {ep+1}, best_val_loss={best_loss:.5f}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "device = \"cpu\"\n",
    "pos_rate = float(np.mean(ytr))\n",
    "pos_weight = (1.0 - pos_rate) / max(pos_rate, 1e-6)\n",
    "\n",
    "gnam = GNAM(n_features=Xtr_t.shape[1], hidden=96, dropout=0.20)\n",
    "tr_loader = to_loader(Xtr_t, ytr, batch=1024, shuffle=True)\n",
    "va_loader = to_loader(Xva_t, yva, batch=2048, shuffle=False)\n",
    "\n",
    "gnam = train_gnam(\n",
    "    gnam, tr_loader, va_loader,\n",
    "    max_epochs=600, lr=5e-4, weight_decay=1e-4, patience=200,\n",
    "    device=device, pos_weight=pos_weight, logit_clamp=10.0, grad_clip_norm=5.0,\n",
    "    report_every=20\n",
    ")\n",
    "\n",
    "torch.save(gnam.state_dict(), INT_DIR/\"gnam_best.pt\")\n",
    "\n",
    "gnam.eval()\n",
    "with torch.no_grad(), amp_ctx:\n",
    "    eta_tr = gnam(torch.tensor(Xtr_t, dtype=torch.float32, device=device)).cpu().numpy().astype(np.float32)\n",
    "    eta_va = gnam(torch.tensor(Xva_t, dtype=torch.float32, device=device)).cpu().numpy().astype(np.float32)\n",
    "    eta_te = gnam(torch.tensor(Xte_t, dtype=torch.float32, device=device)).cpu().numpy().astype(np.float32)\n",
    "\n",
    "p_tr_gnam = sps.expit(eta_tr)\n",
    "p_va_gnam = sps.expit(eta_va)\n",
    "p_te_gnam = sps.expit(eta_te)\n",
    "iso_gnam = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_gnam, yva)\n",
    "p_gnam = iso_gnam.transform(p_te_gnam)\n",
    "\n",
    "print(\"GNAM raw (valid AUROC/AP):\", roc_auc_score(yva, p_va_gnam), average_precision_score(yva, p_va_gnam))\n",
    "print(\"GNAM test (raw AUROC/AP):\", roc_auc_score(yte, p_te_gnam), average_precision_score(yte, p_te_gnam))\n",
    "\n",
    "joblib.dump({\"eta_tr\": eta_tr, \"eta_va\": eta_va, \"eta_te\": eta_te}, INT_DIR/\"gnam_eta.joblib\")\n",
    "joblib.dump(p_gnam, INT_DIR/\"p_gnam.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316c770e-6311-4325-b4b9-a25fc4757713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.63466\ttrain-auc:0.78195\ttrain-logloss:0.53570\tvalid-aucpr:0.63093\tvalid-auc:0.78350\tvalid-logloss:0.53377\n",
      "[200]\ttrain-aucpr:0.67505\ttrain-auc:0.80163\ttrain-logloss:0.51542\tvalid-aucpr:0.65816\tvalid-auc:0.79563\tvalid-logloss:0.52095\n",
      "[400]\ttrain-aucpr:0.69111\ttrain-auc:0.81047\ttrain-logloss:0.50612\tvalid-aucpr:0.66247\tvalid-auc:0.79761\tvalid-logloss:0.51853\n",
      "[600]\ttrain-aucpr:0.70311\ttrain-auc:0.81744\ttrain-logloss:0.49876\tvalid-aucpr:0.66386\tvalid-auc:0.79831\tvalid-logloss:0.51758\n",
      "[800]\ttrain-aucpr:0.71309\ttrain-auc:0.82340\ttrain-logloss:0.49241\tvalid-aucpr:0.66435\tvalid-auc:0.79869\tvalid-logloss:0.51693\n",
      "[1000]\ttrain-aucpr:0.72190\ttrain-auc:0.82858\ttrain-logloss:0.48679\tvalid-aucpr:0.66440\tvalid-auc:0.79858\tvalid-logloss:0.51688\n",
      "[1200]\ttrain-aucpr:0.72953\ttrain-auc:0.83334\ttrain-logloss:0.48162\tvalid-aucpr:0.66437\tvalid-auc:0.79846\tvalid-logloss:0.51687\n",
      "[1400]\ttrain-aucpr:0.73709\ttrain-auc:0.83771\ttrain-logloss:0.47676\tvalid-aucpr:0.66415\tvalid-auc:0.79830\tvalid-logloss:0.51699\n",
      "[1600]\ttrain-aucpr:0.74392\ttrain-auc:0.84194\ttrain-logloss:0.47204\tvalid-aucpr:0.66318\tvalid-auc:0.79794\tvalid-logloss:0.51733\n",
      "[1800]\ttrain-aucpr:0.75025\ttrain-auc:0.84591\ttrain-logloss:0.46757\tvalid-aucpr:0.66238\tvalid-auc:0.79754\tvalid-logloss:0.51767\n",
      "[2000]\ttrain-aucpr:0.75647\ttrain-auc:0.84966\ttrain-logloss:0.46340\tvalid-aucpr:0.66151\tvalid-auc:0.79697\tvalid-logloss:0.51827\n",
      "[2027]\ttrain-aucpr:0.75720\ttrain-auc:0.85013\ttrain-logloss:0.46282\tvalid-aucpr:0.66140\tvalid-auc:0.79689\tvalid-logloss:0.51835\n",
      "GNAM–Boost: AUROC = 0.7977567845679499 AP = 0.6604316160009425 Brier = 0.16946437148611956 LogLoss = 0.507939540320873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Nicee\\\\Desktop\\\\kenkyu\\\\gnamboost_outputs\\\\interim\\\\p_final.npy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 GNAM–Boost (XGB on GNAM base_margin)\n",
    "import xgboost as xgb\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "import numpy as np, joblib\n",
    "\n",
    "etas = joblib.load(INT_DIR/\"gnam_eta.joblib\")\n",
    "eta_tr, eta_va, eta_te = etas[\"eta_tr\"], etas[\"eta_va\"], etas[\"eta_te\"]\n",
    "\n",
    "dtr2 = xgb.DMatrix(Xtr_t, label=ytr); dtr2.set_base_margin(eta_tr.astype(np.float32))\n",
    "dva2 = xgb.DMatrix(Xva_t, label=yva); dva2.set_base_margin(eta_va.astype(np.float32))\n",
    "dte2 = xgb.DMatrix(Xte_t, label=yte); dte2.set_base_margin(eta_te.astype(np.float32))\n",
    "\n",
    "pos_rate=float(ytr.mean())\n",
    "scale_pos_weight=((1.0-pos_rate)/max(pos_rate,1e-9))**0.5\n",
    "\n",
    "params2 = {\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"eval_metric\":[\"aucpr\",\"auc\",\"logloss\"],\n",
    "    \"grow_policy\":\"lossguide\",\"max_depth\":0,\"max_leaves\":36,\"eta\":0.05,\n",
    "    \"subsample\":0.7,\"colsample_bytree\":0.6,\"colsample_bylevel\":0.8,\n",
    "    \"min_child_weight\":15,\"reg_alpha\":0.20,\"reg_lambda\":3.0,\"max_delta_step\":1,\n",
    "    \"scale_pos_weight\":scale_pos_weight,\"tree_method\":\"hist\"\n",
    "}\n",
    "bst2 = xgb.train(params2, dtr2, num_boost_round=6000, evals=[(dtr2,\"train\"),(dva2,\"valid\")],\n",
    "                 early_stopping_rounds=800, verbose_eval=200)\n",
    "\n",
    "best_iter = bst2.best_iteration if hasattr(bst2,\"best_iteration\") else None\n",
    "p_va_gb = bst2.predict(dva2, iteration_range=(0, best_iter+1) if best_iter is not None else None)\n",
    "p_te_gb = bst2.predict(dte2, iteration_range=(0, best_iter+1) if best_iter is not None else None)\n",
    "iso_gb = IsotonicRegression(out_of_bounds=\"clip\").fit(p_va_gb, yva)\n",
    "p_final = iso_gb.transform(p_te_gb)\n",
    "\n",
    "print(\"GNAM–Boost:\",\n",
    "      \"AUROC =\", roc_auc_score(yte,p_final),\n",
    "      \"AP =\", average_precision_score(yte,p_final),\n",
    "      \"Brier =\", brier_score_loss(yte,p_final),\n",
    "      \"LogLoss =\", log_loss(yte,p_final))\n",
    "\n",
    "# Save\n",
    "bst2.save_model(str(INT_DIR/\"gnamboost_bst.json\"))\n",
    "joblib.dump(p_final, INT_DIR/\"p_final.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8037084-1457-4338-aeb7-30b1433dde97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
